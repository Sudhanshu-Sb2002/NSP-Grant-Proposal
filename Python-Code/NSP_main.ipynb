{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "tags": [],
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from Input_function import import_data1\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "#from NeuralNetNSP import Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "outputs": [],
   "source": [
    "path=\"D:\\\\OneDrive - Indian Institute of Science\\\\4th Sem\\\\NSP\\\\NSP-Grant-Proposal\\\\MATLAB-COde\\\\forPython.mat\"\n",
    "[train_attributes,train_labels,test_attributes,test_labels]=import_data1(path)\n",
    "#jupyter nbextension enable varInspector/main"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "outputs": [],
   "source": [
    "import torch as t\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        n0 = 11\n",
    "        n1 = 7\n",
    "        n2 = 4\n",
    "        super(Net, self).__init__()  # calling base class\n",
    "        self.fc1 = nn.Linear(n0, n1)  # input is 11x1\n",
    "        # self.init_weights(self.fc1)\n",
    "        nn.init.xavier_uniform_(self.fc1.weight)\n",
    "        # print(self.fc1.weight)\n",
    "        self.fc2 = nn.Linear(n1, n2)\n",
    "        # self.init_weights(self.fc2)\n",
    "        nn.init.xavier_uniform_(self.fc1.weight)\n",
    "        # print(self.fc2.weight)\n",
    "        self.fc3 = nn.Linear(n2, 1)\n",
    "        # self.init_weights(self.fc3)\n",
    "        nn.init.xavier_uniform_(self.fc1.weight)\n",
    "        # print(self.fc3.weight)\n",
    "\n",
    "    '''def init_weights(self,m):\n",
    "        if isinstance(m):\n",
    "            nn.init.xavier_uniform(m.weight)\n",
    "\n",
    "    '''\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        # print(x)\n",
    "        # assert x.size == n1\n",
    "        x = F.relu(self.fc2(x))\n",
    "        # print(x)\n",
    "        x = t.sigmoid(self.fc3(x))  # change t to f if it doesn't work\n",
    "        # print(x)\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (fc1): Linear(in_features=11, out_features=7, bias=True)\n",
      "  (fc2): Linear(in_features=7, out_features=4, bias=True)\n",
      "  (fc3): Linear(in_features=4, out_features=1, bias=True)\n",
      ")\n",
      "<class 'numpy.float64'>\n"
     ]
    }
   ],
   "source": [
    "net = Net()\n",
    "print(net)\n",
    "params = list(net.parameters())\n",
    "\n",
    "print(type(train_labels[0][0]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'int' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Input \u001B[1;32mIn [100]\u001B[0m, in \u001B[0;36m<cell line: 3>\u001B[1;34m()\u001B[0m\n\u001B[0;32m      7\u001B[0m target\u001B[38;5;241m=\u001B[39mtrain_labels\n\u001B[0;32m      8\u001B[0m criterion \u001B[38;5;241m=\u001B[39m nn\u001B[38;5;241m.\u001B[39mMSELoss()\n\u001B[1;32m----> 9\u001B[0m loss \u001B[38;5;241m=\u001B[39m \u001B[43mcriterion\u001B[49m\u001B[43m(\u001B[49m\u001B[43moutput\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtarget\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     10\u001B[0m loss\u001B[38;5;241m.\u001B[39mbackward()\n\u001B[0;32m     11\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mstep()\n",
      "File \u001B[1;32mC:\\ProgramData\\Anaconda3\\envs\\NSP-code\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1106\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1107\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1108\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1109\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1110\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39m\u001B[38;5;28minput\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1111\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1112\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32mC:\\ProgramData\\Anaconda3\\envs\\NSP-code\\lib\\site-packages\\torch\\nn\\modules\\loss.py:529\u001B[0m, in \u001B[0;36mMSELoss.forward\u001B[1;34m(self, input, target)\u001B[0m\n\u001B[0;32m    528\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor, target: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[1;32m--> 529\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmse_loss\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtarget\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreduction\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreduction\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mC:\\ProgramData\\Anaconda3\\envs\\NSP-code\\lib\\site-packages\\torch\\nn\\functional.py:3251\u001B[0m, in \u001B[0;36mmse_loss\u001B[1;34m(input, target, size_average, reduce, reduction)\u001B[0m\n\u001B[0;32m   3247\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_variadic(\u001B[38;5;28minput\u001B[39m, target):\n\u001B[0;32m   3248\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[0;32m   3249\u001B[0m         mse_loss, (\u001B[38;5;28minput\u001B[39m, target), \u001B[38;5;28minput\u001B[39m, target, size_average\u001B[38;5;241m=\u001B[39msize_average, reduce\u001B[38;5;241m=\u001B[39mreduce, reduction\u001B[38;5;241m=\u001B[39mreduction\n\u001B[0;32m   3250\u001B[0m     )\n\u001B[1;32m-> 3251\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[43mtarget\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msize\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;241m==\u001B[39m \u001B[38;5;28minput\u001B[39m\u001B[38;5;241m.\u001B[39msize()):\n\u001B[0;32m   3252\u001B[0m     warnings\u001B[38;5;241m.\u001B[39mwarn(\n\u001B[0;32m   3253\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUsing a target size (\u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m) that is different to the input size (\u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m). \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   3254\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mThis will likely lead to incorrect results due to broadcasting. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   3255\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPlease ensure they have the same size.\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(target\u001B[38;5;241m.\u001B[39msize(), \u001B[38;5;28minput\u001B[39m\u001B[38;5;241m.\u001B[39msize()),\n\u001B[0;32m   3256\u001B[0m         stacklevel\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m2\u001B[39m,\n\u001B[0;32m   3257\u001B[0m     )\n\u001B[0;32m   3258\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m size_average \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mor\u001B[39;00m reduce \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "\u001B[1;31mTypeError\u001B[0m: 'int' object is not callable"
     ]
    }
   ],
   "source": [
    "optimizer = optim.SGD(net.parameters(), lr=0.01)\n",
    "num_epochs = 100\n",
    "\n",
    "for i in range(num_epochs):\n",
    "    optimizer.zero_grad()\n",
    "    inp=t.tensor(train_attributes).float()\n",
    "    output =net(inp)\n",
    "    target=train_labels\n",
    "    criterion = nn.MSELoss()\n",
    "    loss = criterion(output, target)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(loss)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}